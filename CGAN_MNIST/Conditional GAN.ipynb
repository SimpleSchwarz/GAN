{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6692b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ec422",
   "metadata": {},
   "source": [
    "\"Import libraries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0a3652e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cganG(nn.Module):\n",
    "    def __init__(self, classes, channels, img_size, latent_dim):\n",
    "        super(cganG, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.channels = channels\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self._create_layer(self.latent_dim + self.classes, 128, False),\n",
    "            *self._create_layer(128, 256),\n",
    "            *self._create_layer(256, 512),\n",
    "            *self._create_layer(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _create_layer(self, size_in, size_out, normalize=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        z = torch.cat((self.label_embedding(labels), noise), -1)\n",
    "        x = self.model(z)\n",
    "        x = x.view(x.size(0), *self.img_shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f45d9",
   "metadata": {},
   "source": [
    "\"Define the generator\"\n",
    "#The generator class consists for three functions by using torch.nn.Module which help you build your network models easily: __init__, _create_layer,and forward.\n",
    "#The __init__ method is where we typically define the attributes of a class. You can do any setup here. I set the number of classes, the number of channels, the size of image, the dimension of latent vector, and the nn.Embedding module. This module is  simple lookup table that stores embeddings of a fixed dictionary and size. It is used to process the label information with the random latent vector.\n",
    "#_create_layer is where we define layers. It consists of 5 linear layers, 3 of which are connected to batch normalization layers, and the first 4 linear layers have LeakyReLu activation functions while the last has a Tahn activation function. Batch normaliazation is a method for the extracted features in the hidden units to make training faster and more stable.\n",
    "#The forward method is called when we use the neural network to make a prediction. torch.cat is used to concatenate the given sequence of seq tensors in the given dimension, -1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de355e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cganD(nn.Module):\n",
    "    def __init__(self, classes, channels, img_size, latent_dim):\n",
    "        super(cganD, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.channels = channels\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "        self.adv_loss = torch.nn.BCELoss()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self._create_layer(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
    "            *self._create_layer(1024, 512, True, True),\n",
    "            *self._create_layer(512, 256, True, True),\n",
    "            *self._create_layer(256, 128, False, False),\n",
    "            *self._create_layer(128, 1, False, False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _create_layer(self, size_in, size_out, drop_out=True, act_func=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(0.4))\n",
    "        if act_func:\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, image, labels):\n",
    "        x = torch.cat((image.view(image.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7310f1b",
   "metadata": {},
   "source": [
    "\"Define the discriminator\"\n",
    "The discriminator outputs a single value to show how close an image is to the real images as given the label information. The network consists of 5 linear layers, 2 of which are connected to dropout layers to to prevent overfitting. It makes all the nodes work well as a team by making sure no node is too weak or too strong through some neurons are not included in a particular forward or backward pass. BCE loss function is typically used for the binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6455e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 data_loader,\n",
    "                 classes,\n",
    "                 channels,\n",
    "                 img_size,\n",
    "                 latent_dim):\n",
    "        self.device = device\n",
    "        self.data_loader = data_loader\n",
    "        self.classes = classes\n",
    "        self.channels = channels\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.netG = cganG(self.classes, self.channels, self.img_size, self.latent_dim)\n",
    "        self.netG.to(self.device)\n",
    "        self.netD = cganD(self.classes, self.channels, self.img_size, self.latent_dim)\n",
    "        self.netD.to(self.device)\n",
    "        self.optim_G = None\n",
    "        self.optim_D = None\n",
    "\n",
    "    def create_optim(self, lr, alpha=0.5, beta=0.999):\n",
    "        self.optim_G = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                        self.netG.parameters()),\n",
    "                                        lr=lr,\n",
    "                                        betas=(alpha, beta))\n",
    "        self.optim_D = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                        self.netD.parameters()),\n",
    "                                        lr=lr,\n",
    "                                        betas=(alpha, beta))\n",
    "\n",
    "    def train(self,\n",
    "              epochs,\n",
    "              log_interval,\n",
    "              out_dir=''):\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        viz_noise = torch.randn(self.data_loader.batch_size, self.latent_dim, device=self.device)\n",
    "        nrows = self.data_loader.batch_size // 8\n",
    "        viz_label = torch.LongTensor(np.array([num for _ in range(nrows) for num in range(8)])).to(self.device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_time = time.time()\n",
    "            for batch_idx, (data, target) in enumerate(self.data_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                batch_size = data.size(0)\n",
    "                real_label = torch.full((batch_size, 1), 1., device=self.device)\n",
    "                fake_label = torch.full((batch_size, 1), 0., device=self.device)\n",
    "\n",
    "                # Train G\n",
    "                self.netG.zero_grad()\n",
    "                z_noise = torch.randn(batch_size, self.latent_dim, device=self.device)\n",
    "                x_fake_labels = torch.randint(0, self.classes, (batch_size,), device=self.device)\n",
    "                x_fake = self.netG(z_noise, x_fake_labels)\n",
    "                y_fake_g = self.netD(x_fake, x_fake_labels)\n",
    "                g_loss = self.netD.loss(y_fake_g, real_label)\n",
    "                g_loss.backward()\n",
    "                self.optim_G.step()\n",
    "\n",
    "                # Train D\n",
    "                self.netD.zero_grad()\n",
    "                y_real = self.netD(data, target)\n",
    "                d_real_loss = self.netD.loss(y_real, real_label)\n",
    "                y_fake_d = self.netD(x_fake.detach(), x_fake_labels)\n",
    "                d_fake_loss = self.netD.loss(y_fake_d, fake_label)\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                self.optim_D.step()\n",
    "\n",
    "                if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "                    print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(\n",
    "                              epoch, batch_idx, len(self.data_loader),\n",
    "                              d_loss.mean().item(),\n",
    "                              g_loss.mean().item(),\n",
    "                              time.time() - batch_time))\n",
    "                    #Show a real image\n",
    "                    #vutils.save_image(data, os.path.join(out_dir, 'real_samples.png'), normalize=True)\n",
    "                    with torch.no_grad():\n",
    "                        viz_sample = self.netG(viz_noise, viz_label)\n",
    "                        #Show a fake image\n",
    "                        #vutils.save_image(viz_sample, os.path.join(out_dir, 'fake_samples_{}.png'.format(epoch)), nrow=8, normalize=True)\n",
    "                    batch_time = time.time()\n",
    "\n",
    "            #self.save_to(path=out_dir, name=self.name, verbose=False)\n",
    "            print('Total train time: {:.2f}'.format(time.time() - total_time))\n",
    "\n",
    "    def eval(self,\n",
    "             mode=None,\n",
    "             batch_size=None):\n",
    "        self.netG.eval()\n",
    "        self.netD.eval()\n",
    "        if batch_size is None:\n",
    "            batch_size = self.data_loader.batch_size\n",
    "        nrows = batch_size // 8\n",
    "        viz_labels = np.array([num for _ in range(nrows) for num in range(8)])\n",
    "        viz_labels = torch.LongTensor(viz_labels).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            viz_tensor = torch.randn(batch_size, self.latent_dim, 1, 1, device=self.device)\n",
    "            viz_sample = self.netG(viz_tensor, viz_labels)\n",
    "            viz_vector = utils.to_np(viz_tensor).reshape(batch_size, self.latent_dim)\n",
    "            cur_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            np.savetxt('vec_{}.txt'.format(cur_time), viz_vector)\n",
    "            vutils.save_image(viz_sample, 'img_{}.png'.format(cur_time), nrow=8, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5839c",
   "metadata": {},
   "source": [
    "\"Define model training\"\n",
    "#__init__: the generator(netG) and the discriminator(netD) are initialized based on the class number(classes), image channel(channels), image size(img_size_), and the length of the latent vector(latent_dim). The optim_G and optim_D are optimizers for the two networks.\n",
    "#create_optim: define optimizers.\n",
    "#train: parameters of the train funcntion are the number of training epochs(epochs), the message interval you can check results during training(log_interval), and the output directory(out_dir). self.netG.train() and self.netD.train() turn on train mode. Then, latent vectors (viz_nois) and label (viz_label) are defined. They are used to occasionally produce images during training so that we can track how the model is trained. Torch.randn returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1. Torch.LongTensor defines 64-bit integer(signed) as a data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf17fcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --model: conflicting option string: --model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15880/48980993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munknown\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cgan'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'one of `cgan` and `infogan`.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--cuda'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboolean_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'enable CUDA.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboolean_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train mode or eval mode.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1799\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1800\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1801\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1802\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1639\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1448\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;31m# resolve any conflicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1450\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;31m# add to actions list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1586\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1587\u001b[1;33m             \u001b[0mconflict_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1589\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[1;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[0;32m   1594\u001b[0m                                      \u001b[1;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1595\u001b[0m                                      in conflicting_actions])\n\u001b[1;32m-> 1596\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --model: conflicting option string: --model"
     ]
    }
   ],
   "source": [
    "FLAGS = None\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if FLAGS.cuda else \"cpu\")\n",
    "\n",
    "    if FLAGS.train:\n",
    "        print('Loading data...\\n')\n",
    "        dataset = dset.MNIST(root=FLAGS.data_dir, download=False,\n",
    "                             transform=transforms.Compose([\n",
    "                             transforms.Resize(FLAGS.img_size),\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.5,), (0.5,))\n",
    "                             ]))\n",
    "        assert dataset\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=FLAGS.batch_size,\n",
    "                                                 shuffle=True, num_workers=4, pin_memory=True)\n",
    "        print('Creating model...\\n')\n",
    "        model = Model(FLAGS.model, device, dataloader, FLAGS.classes, FLAGS.channels, FLAGS.img_size, FLAGS.latent_dim)\n",
    "        model.create_optim(FLAGS.lr)\n",
    "\n",
    "        # Train\n",
    "        model.train(FLAGS.epochs, FLAGS.log_interval, FLAGS.out_dir, True)\n",
    "\n",
    "        model.save_to('')\n",
    "    else:\n",
    "        model = Model(FLAGS.model, device, None, FLAGS.classes, FLAGS.channels, FLAGS.img_size, FLAGS.latent_dim)\n",
    "        model.load_from(FLAGS.out_dir)\n",
    "        model.eval(mode=1, batch_size=FLAGS.batch_size)   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    parser.add_argument('--model', type=str, default='cgan', help='one of `cgan` and `infogan`.')\n",
    "    parser.add_argument('--cuda', type=boolean_string, default=True, help='enable CUDA.')\n",
    "    parser.add_argument('--train', type=boolean_string, default=True, help='train mode or eval mode.')\n",
    "    parser.add_argument('--data_dir', type=str, default='~/data/MNIST', help='Directory for dataset.')\n",
    "    parser.add_argument('--out_dir', type=str, default='output', help='Directory for output.')\n",
    "    parser.add_argument('--epochs', type=int, default=200, help='number of epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='size of batches')\n",
    "    parser.add_argument('--lr', type=float, default=0.0002, help='learning rate')\n",
    "    parser.add_argument('--latent_dim', type=int, default=100, help='latent space dimension')\n",
    "    parser.add_argument('--classes', type=int, default=10, help='number of classes')\n",
    "    parser.add_argument('--img_size', type=int, default=64, help='size of images')\n",
    "    parser.add_argument('--channels', type=int, default=1, help='number of image channels')\n",
    "    parser.add_argument('--log_interval', type=int, default=100, help='interval between logging and image sampling')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "\n",
    "    FLAGS = parser.parse_args()\n",
    "    FLAGS.cuda = FLAGS.cuda and torch.cuda.is_available()\n",
    "\n",
    "    if FLAGS.seed is not None:\n",
    "        torch.manual_seed(FLAGS.seed)\n",
    "        if FLAGS.cuda:\n",
    "            torch.cuda.manual_seed(FLAGS.seed)\n",
    "        np.random.seed(FLAGS.seed)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    if FLAGS.train:\n",
    "        utils.clear_folder(FLAGS.out_dir)\n",
    "\n",
    "    log_file = os.path.join(FLAGS.out_dir, 'log.txt')\n",
    "    print(\"Logging to {}\\n\".format(log_file))\n",
    "    sys.stdout = utils.StdOut(log_file)\n",
    "\n",
    "    print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
    "\n",
    "    print(\" \" * 9 + \"Args\" + \" \" * 9 + \"|    \" + \"Type\" + \\\n",
    "          \"    |    \" + \"Value\")\n",
    "    print(\"-\" * 50)\n",
    "    for arg in vars(FLAGS):\n",
    "        arg_str = str(arg)\n",
    "        var_str = str(getattr(FLAGS, arg))\n",
    "        type_str = str(type(getattr(FLAGS, arg)).__name__)\n",
    "        print(\"  \" + arg_str + \" \" * (20-len(arg_str)) + \"|\" + \\\n",
    "              \"  \" + type_str + \" \" * (10-len(type_str)) + \"|\" + \\\n",
    "              \"  \" + var_str)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f72befb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "img_size = 64\n",
    "batch_size = 64\n",
    "classes = 10\n",
    "device = torch.device(\"cpu\")\n",
    "channels = 1\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9db1b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(root=data_dir, download=False,\n",
    "                    transform=transforms.Compose([\n",
    "                    transforms.Resize(img_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,))\n",
    "                    ]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "model = Model(FLAGS.model, device, dataloader, FLAGS.classes, FLAGS.channels, FLAGS.img_size, FLAGS.latent_dim)\n",
    "#model.create_optim(FLAGS.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6640a7d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15880/2641310087.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnetD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "abfb74f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Model at 0x1ffc4981bb0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model(device, dataloader, classes, channels, img_size, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b91a4b12",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15880/1695128209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "Model.train(epochs=10, log_interval=100, out_dir='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20fc13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestClass:\n",
    "    def __init__(self):\n",
    "        print(\"in init\")\n",
    "    def testFunc(self):\n",
    "        print(\"in Test Func\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6285d4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in init\n"
     ]
    }
   ],
   "source": [
    "testInstance = TestClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00165da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in Test Func\n"
     ]
    }
   ],
   "source": [
    "testInstance.testFunc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6adbb",
   "metadata": {},
   "source": [
    "\"Run the model\"\n",
    "#main(): The FLAGS object stores all the arguments and hyper-parameters needed for model definition and training. To make the configuration of the arguments more user-friendly, we will use the argparse module provided by Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
